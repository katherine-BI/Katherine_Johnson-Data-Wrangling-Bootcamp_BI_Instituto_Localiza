# -*- coding: utf-8 -*-
"""pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aLdqQv1qx26bdzvO3cipej38XiVEPGft
"""

import pandas as pd
import numpy as np
import sqlite3 as sql

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""#Desafio: Data Wrangling & Pipeline de Dados


*Contextualização*

Você foi contratada como Analista de BI Júnior para ajudar uma startup que coleta dados de vendas online.
Eles disponibilizaram um dataset bruto em formato CSV, mas o arquivo contém inconsistências.
Seu desafio é limpar, transformar e carregar os dados em um banco de dados SQLite, criando um pipeline em Python.

⚠️ Problemas identificados no dataset:
* Datas em formatos diferentes.
* Valores ausentes em categoria.
* Quantidade em texto (“três”).
* Preços negativos.
* Cliente nulo.

##Aquecimento: Exploando e Entendendo os Dados

Antes de começar o desafio, responda às perguntas abaixo para revisar conceitos e se preparar para construit seu pipeline de dados:

**Leitura e Exploração Inicial**
1. Leia o arquivo CSV e visualize as 5 primeira linhas do dataset
2. Verifique o número de linhas e colunas
3. Descubra o tipo de dados (dtype) de cada coluna
4. Conte os valores ausentes existentes em cada coluna

**Seleção e Filtragem de Dados**
1. Filtre apenas as linhas onde o preço unitário é maior que 100
2. Ordene o dataset pelo valor do preço em ordem decrescente
"""

df = pd.read_csv('/content/drive/MyDrive/Desafio Data Wrangling/vendas.csv')

"""##Leitura e Exploração"""

df.head()

df.shape

df.info()

df.dtypes

#identificando valores ausentes
print("Valores ausentes por coluna:")
print(df.isnull().sum())
print("\nTotal de valores ausentes no DataFrame:")
print(df.isnull().sum().sum())

"""##Seleção e Filtragem"""

#Apenas linhas maiores que 100
df100 = df[df['preco_unitario'] > 100]
display(df100.head())

df100.shape

#Ordenando o df em ordem decrescente
df_ordenado = df.sort_values(by='preco_unitario', ascending=False)

df_ordenado.head()

"""#Iniciando o desafio

##Objetivo do Desafio: Criar um pipeline em Python que siga estas etapas

**1. Extração (E)**

Ler o CSV com os dados de vendas

**2. Transformação (T)**

* Padronizar a coluna de data (YYYY-MM-DD);
* Substituir valores nulos por "Não informado";
* Corrigir `quantidade` para ser sempre número inteiro;
* Remover ou corrigir preços negativos;
* Criar uma nova coluna `valor_total = quantidade * preco_unitário`

**3.Carga (L)**

* Criar um banco SQLite (arquivo vendas.db);
* Salvar os dados tratados em uma tabela tb_vendas

*Desafio Extra*
* Criar uma segunda tabela tb_clientes contendo apenas clientes únicos.
* Relacionar tb_vendas com tb_clientes via chave estrangeira.
* Escrever uma consulta SQL que mostre: total de vendas por
categoria.

-- Entregáveis:
1. Código Python (pipeline.py).
2. Arquivo do banco SQLite (vendas.db).
3. Pequeno README explicando como rodar o script.

###Extração
"""

df.head()

df.info ()

df.describe(include='all')

"""###Transformação"""

#Padronizar a coluna de data (YYYY-MM-DD)

def padronizar_data(valor):
    formatos = ['%Y/%m/%d', '%Y-%m-%d', '%d-%m-%Y']
    for formato in formatos:
        try:
            return pd.to_datetime(valor, format=formato)
        except:
            continue
    return pd.NaT
df['data_venda'] = df['data_venda'].apply(padronizar_data)
df['data_venda'].unique()


if df['data_venda'].isnull().any():
    print("Ainda existem valores que não puderam ser convertidos para o formato de data.")
    print("Exemplos de valores não convertidos:")
    display(df[df['data_venda'].isnull()].head())
else:
    print("Coluna 'data_venda' padronizada com sucesso para YYYY-MM-DD.")
    display(df.head())

#Identificando valores ausentes
print("Valores ausentes por coluna:")
print(df.isnull().sum())

#Filtrando e substituindo valores ausentes por "Não informado."
df.fillna('Não informado', inplace=True)
df.isna().any()

# Corrigir quantidade para ser sempre número inteiro
quantidade_map = {
    'um': 1, 'dois': 2, 'três': 3, 'quatro': 4, 'cinco': 5,
    'seis': 6, 'sete': 7, 'oito': 8, 'nove': 9, 'dez': 10
}

df['quantidade'] = df['quantidade'].replace(quantidade_map)

df['quantidade'] = pd.to_numeric(df['quantidade'], errors='coerce')

df['quantidade'].fillna(0)

df['quantidade'] = df['quantidade'].astype(int)

print("Coluna 'quantidade' ajustada para tipo inteiro.")

#identificando se existem preços negativos
df[df['preco_unitario'] < 0]

# Remove o sinal negativo dos preços
df['preco_unitario'] = df['preco_unitario'].abs()

print("Valores negativos removidos da coluna 'preco_unitario'.")

#conferindo se ainda existem números negativos
display(df[df['preco_unitario'] < 0])

#Nova coluna 'valor_total'
df['valor_total'] = df['quantidade'] * df['preco_unitario']

df.head()

"""###Carga (com desafio bônus)"""

#Criação de um novo dataset com clientes únicos
df_clientes = (
    df[['cliente']].drop_duplicates()
    .rename(columns={'cliente': 'nome'})
    .reset_index(drop=True))

#Criação de coluna id_cliente com atribuição 0 para Não informado + ordenação do dataset.

df_clientes['id_cliente'] = np.where(df_clientes['nome'] == 'Não informado', 0, df_clientes.index + 1)
df_clientes = df_clientes.sort_values(by='id_cliente').reset_index(drop=True)

df_clientes

#Inserção da coluna cliente_id no df original com merge, sem criar colunas duplicadas

df = df.merge(df_clientes, left_on='cliente', right_on='nome', how='left').drop(columns=['nome', 'cliente'])
df.head()

#Reordenação de colunas

cols = list(df.columns)
cols.remove('id_cliente')
cols.insert(1, 'id_cliente')
df = df[cols]
df.head()

#Cria a conexão, o arquivo e o cursor, ativando as chaves estrangeiras

conexao = sql.connect("vendas.db")
cursor = conexao.cursor()
cursor.execute("PRAGMA foreign_keys = ON;")

#Criação e população de tb_clientes (desafio extra)

cursor.execute("DROP TABLE IF EXISTS tb_clientes")
cursor.execute("""
CREATE TABLE tb_clientes (
    id_cliente INTEGER PRIMARY KEY,
    nome TEXT
    )""");
df_clientes.to_sql("tb_clientes", conexao, if_exists="append", index=False)

#Criação e população da tabela tb_vendas (desafio principal)

cursor.execute("DROP TABLE IF EXISTS tb_vendas")
cursor.execute("""
CREATE TABLE IF NOT EXISTS tb_vendas (
    id_venda INTEGER PRIMARY KEY,
    id_cliente INTEGER,
    data_venda TEXT,
    produto TEXT,
    quantidade INTEGER,
    preco_unitario REAL,
    categoria TEXT,
    valor_total REAL,
    FOREIGN KEY (id_cliente) REFERENCES tb_clientes(id_cliente)
)
""")

df.to_sql("tb_vendas", conexao, if_exists="append", index=False)

#Commit das alterações

conexao.commit()

#Consulta SQL mostrando total de vendas por categoria

cursor.execute("""
SELECT categoria, SUM(valor_total) AS soma_total_vendas
FROM tb_vendas
GROUP BY categoria
""")

resultados = cursor.fetchall()
resultados

#Encerra a conexão com o banco

conexao.close()